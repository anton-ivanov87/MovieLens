---
title: "MovieLens  \nCreating a movie recommendation system"
author: "Anton Ivanov"
date: "`r format(Sys.Date())`"
output:
  bookdown::html_document2:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Script adding numeration to equations
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { 
      equationNumbers: { 
            autoNumber: "all",
            formatNumber: function (n) {return +n}
      } 
  }
});
</script>




# Introduction

In this project, we will be creating a movie recommendation system using the [10M version of the MovieLens dataset](https://grouplens.org/datasets/movielens/10m/).
This dataset contains 10 million ratings for about 10,000 movies from 72,000 users.

Recommendation systems make specific recommendations based on ratings from users to items. They are of particular importance in the modern world, since they allow companies such as online shops or streaming services to recommend users matching goods or content, providing higher profits. 

The goal of the project is to predict a rating for a specific movie from a specific user, based on the available ratings.
The used ratings are as following: one star suggests it is not a good movie, whereas five stars suggest it is an excellent movie.

Since in the dataset only 10 million ratings are available out of potential 78 million (if every user would rate every movie), about 86% of ratings remain unknown and have to be predicted.

The recommendation system will be based on a linear model containing movie, user, genre and time effects. Each effect will be considered step-by-step, using a training and test sets for evaluation of the model performance. Regularization of the effects will be then implemented to improve the performance. After final tuning of the model, it will be applied to the validation set to assess the final performance.


# Analysis and Methods

## Data preparation

The 10M Movielens dataset is available for a downloaded as a \*.zip file containing two \*.dat files: movies.dat and ratings.dat. The files contain information to user id, movie id, rating (0.5 - 5 stars), time stamp (as number of seconds after 1st January 1970), title of the movie with release year and genre of the movie, separated by double colons "::". The values are extracted and saved as a dataframe using the following code.

```{r dataset preparation, message=FALSE, warning=FALSE}
library(tidyverse)
library(data.table)
dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")
```

In the resulting dataframe, each row represents a rating by one user to one movie. These are the first 10 rows of the dataframe:

```{r 10 rows of dataset, message=FALSE}
as_tibble(movielens)[1:10,]
```


## Preparation of the training, test and validation sets

For the final evaluation of the recommendation systems performance, a validation dataset containing 10% of data is created. The rest of the data is saved as a edx subset. It is ensured that the userId and movieId values in the validation set are also present in the edx set.

```{r validation dataset, message=FALSE, warning=FALSE}
library(caret)
set.seed(1, sample.kind="Rounding")
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)
```

The edx dataset is used for the creation and training of the recommendation system and is split into the training set (80%) and test set (20%) for these needs. 

```{r train and test sets}
set.seed(2)
test_index <- createDataPartition(edx$rating, times = 1, p = 0.2, list = FALSE)
train_set <- edx[-test_index]
test_set <- edx[test_index]
test_set <- test_set %>% 
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")
```

## Creating a linear model

Each movie is rated by a different number of users since some movies are more popular and others are less popular. It can be seen in the following plot (movies with number of ratings less than 5000 are considered).

```{r movie ratings number, echo=FALSE, out.height=70%}
movielens %>%
  group_by(movieId) %>%
  summarize(n = n()) %>%
  filter(n < 5000) %>%
  ggplot(aes(n)) +
  geom_histogram(bins = 20)
```
The simplest model would be assuming all unknown ratings equal to the average rating of all movies. It can be presented with the following equation.

$$
Y_{u,i} = \mu + \epsilon_{u,i}
$$

where $Y_{u,i}$ is the predicted rating, $\mu$ is a mean of all ratings in the data set and $\epsilon_{u,i}$ is an independent error.

Of course such assumption would not result in an accurate prediction, as will be shown in Section \@ref(Results). To improve the prediction, an effet of each specific movie on its rating can be considered. It is obvious that each movie has a different "true" rating - there are objectively good and bad movies. This can be supported by the following plot showing mean ratings of the movies with more than 500 ratings. No pattern can be observed.

```{r}
movielens %>%
  group_by(movieId) %>%
  summarize(mean_i = mean(rating), n = n()) %>%
  filter(n > 500) %>%
  ggplot(aes(seq(1, length(movieId)), mean_i)) +
  geom_point()
```

This fact can be used to predict ratings. The model will include the average of all movies in the dataset plus the specific term for each movie.

$$
Y_{u,i} = \mu + b_i + \epsilon_{u,i}
$$

where $b_i$ is the item (movie) effect.

Analogously, similar relationships can be shown for the overall ratings of one user. Each user shows a different overall average rating. This means that some users give on average higher ratings than other users. The following plot depicts mean ratings of users with more than 500 ratings. As expected, no pattern can be observed.

```{r}
movielens %>%
  group_by(userId) %>%
  summarize(mean_u = mean(rating), n = n()) %>%
  filter(n > 500) %>%
  ggplot(aes(seq(1, length(userId)), mean_u)) +
  geom_point()
```

Thus, another term can be included in the model - the user effect.

$$
Y_{u,i} = \mu + b_i + b_u + \epsilon_{u,i}
$$
where $b_u$ is the user effect.

The shown approach can be implemented also to the genres of movies. This is less intuitive, but it is easy to show the some genres get on average higher ratings than the others.

These are the top 5 rated genres:

```{r}
movielens %>%
  group_by(genres) %>%
  summarize(mean_g = mean(rating), n = n()) %>%
  filter(n > 500) %>%
  slice_max(mean_g, n = 5)
```


And the bottom 5 rated genres:

```{r}
movielens %>%
  group_by(genres) %>%
  summarize(mean_g = mean(rating), n = n()) %>%
  filter(n > 500) %>%
  slice_min(mean_g, n = 5)
```

Accordingly, the genre effect is also included in the model.

$$
Y_{u,i} = \mu + b_i + b_u + b_g + \epsilon_{u,i}
$$
where $b_g$ is the genre effect.

The last bit of available information in the dataset is the year of the movie release (included in the title) and the date when the rating was given. This information itself might not be very helpful. However, according to our experience, the time __between__ the movie release and the rating date might be important. It seems that older movie being rated after many years are rated more positive. To check this hypothesis, the dataset will be first adjusted to include the new columns: movie date, rating date and the difference in years between them. Afterwards, the ratings will be plotted against the time difference for a sample of 100000 entries.

```{r adding year to movielense, message=FALSE}
library(lubridate)

movielens <- movielens %>%
  mutate(year_m = str_extract(title, "\\(\\d{4}\\)")) %>%
  mutate(year_m = str_extract(year_m, "\\d{4}")) %>%
  mutate(year_m = as.numeric(year_m),
         year_r = year(as.POSIXct(timestamp, origin="1970-01-01")),
         year_diff = year_r - year_m)
```


```{r plot year_diff to rating, message=FALSE}
movielens[sample(nrow(movielens), 1000000),] %>%
  group_by(year_diff) %>%
  summarize(rating_mean = mean(rating)) %>%
  ggplot(aes(year_diff, rating_mean)) +
  geom_point() +
  geom_smooth(method = "lm")
```

As can be seen, indeed, there is a positive correlation between the movie date and rating date difference and the movie rating. Similarly to the previous approach, this information will be included in the model via the time effect $b_y$.

$$
Y_{u,i} = \mu + b_i + b_u + b_g + b_y + \epsilon_{u,i}
$$

Now the model includes all useful information available in the dataset. But how do we calculate the effects?

## Calculation of effects

We will perform the calculation of all effects step by step starting with the movie effect $b_i$. From the equation 2, the $b_i$ specific for each movie can be approximated based on the average rating of all movies and the known ratings of specific a movie.

$$
\hat{b_i} = \sum_{u=1}^{n_i}(Y_{i,u} - \hat{\mu})
$$

whereas terms with hats are estimated values.

Knowing $b_i$ and rewriting the equation 3 the effect $b_u$ can be found as following. 

$$
\hat{b_u} = \sum_{u=1}^{n_i}(Y_{i,u} - \hat{\mu} - \hat{b_i})
$$

In the same manner, the values of the last two effects can be found.

$$
\hat{b_g} = \sum_{u=1}^{n_i}(Y_{i,u} - \hat{\mu} - \hat{b_i} - \hat{b_u})
$$
$$
\hat{b_y} = \sum_{u=1}^{n_i}(Y_{i,u} - \hat{\mu} - \hat{b_i} - \hat{b_u} - \hat{b_g})
$$


## Calculation of effects with regularization

As shown in equations __X-X__, calculation of effects depends on averages of ratings grouped in different ways (ratings for the same movie, of the same user, of the same genre or of the same movie date and rating date difference). This means that the more ratings of one group are available, the better estimation of an effect is possible. To compensate a strong influence of low number of ratings, a penalty term $\lambda$ is introduced into the equations for the effects. Penalizing large estimates that are formed using small sample sizes is the concept of **regularization**.

For example, for the movie effect $b_i$, the modified equation looks as following:

$$
\hat{b_i}(\lambda) = \frac{1}{\lambda + n_i}\sum_{u=1}^{n_i}(Y_{i,u} - \hat{\mu})
$$

As can be seen, when the sample size $n_i$ is large giving a stable estimate of $\hat{b_i}$, the penalty term $\lambda$ is ignored and $n_i + \lambda \approx n_i$. By a small sample size $\lambda$ becomes dominant and $\hat{b_i}(\lambda)§ is shrunken towards 0.

In a similar way, penalty terms will be added to each effect. Optimal penalty terms will be defined by trying different values and comparing their influence on the performance of the prediction. 

## Loss function

The performance of the prediction system will be measured by the typical error loss - the residual mean squared error (RMSE) on a test set.

$$
RMSE = \sqrt{\frac{1}{N}\sum_{u,i}(\hat{y}_{u,i}-y_{u,i})^2}
$$

where $y_{u,i}$ is an actual rating of user __u__ for movie __i__, $\hat{y}_{u,i}$ is the corresponding predicted value and __N__ is  the number of user/movie combinations.

The function that will compute the RMSE for vectors of ratings and their corresponding predictors:

```{r RMSE}
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
```


# Results

In the following, the described model will be built and the effects and regularization penalty terms will be calculated based on the train and test sets. The final performance of the recommendation system will be evaluated with the validation set.

## Model without regularization

First, we will try just to randomly guess the ratings and show the corresponding RMSE:

```{r guessing}
set.seed(3)
pred_random <- sample(seq(0.5, 5, by = 0.5), nrow(test_set), replace = TRUE)
rmse_random <- RMSE(test_set$rating, pred_random)
rmse_random
```
Not surprisingly, the RMSE is quite high. Just applying our simplest model --- assuming all ratings to be the average of the known ratings --- already improves the performance.

```{r just average}
mu <- mean(train_set$rating)
rmse_mean <- RMSE(test_set$rating, mu)
rmse_mean
```
Now we will add the first effect (movie effect) to the model. The effect is calculated according to the equation 6.

```{r movie effect}
b_i <- train_set %>% 
  group_by(movieId) %>%
  summarize(b_i = sum(rating - mu)/n())

predicted_ratings_b_i <- test_set %>% 
  left_join(b_i, by = "movieId") %>%
  mutate(pred = mu + b_i) %>%
  pull(pred)

rmse_b_i <- RMSE(test_set$rating, predicted_ratings_b_i)
rmse_b_i
```
Addition of the movie effect results in the improvement of the prediction on the test set of 11%. Implementation of the other effects in the model and their influence on the model performance are shown in the following.

User effect:

```{r user effect}
b_u <- train_set %>% 
  left_join(b_i, by="movieId") %>%
  group_by(userId) %>%
  summarize(b_u = sum(rating - b_i - mu)/n())

predicted_ratings_b_iu <- test_set %>% 
  left_join(b_i, by = "movieId") %>%
  left_join(b_u, by = "userId") %>%
  mutate(pred = mu + b_i + b_u) %>%
  pull(pred)

rmse_b_iu <- RMSE(test_set$rating, predicted_ratings_b_iu)
rmse_b_iu
```
Genre effect:

```{r genre effect}
b_g <- train_set %>% 
  left_join(b_i, by="movieId") %>%
  left_join(b_u, by = "userId") %>%
  group_by(genres) %>%
  summarize(b_g = sum(rating - b_i - b_u - mu)/n())

predicted_ratings_b_iug <- 
  test_set %>% 
  left_join(b_i, by = "movieId") %>%
  left_join(b_u, by = "userId") %>%
  left_join(b_g, by = "genres") %>%
  mutate(pred = mu + b_i + b_u + b_g) %>%
  pull(pred)

rmse_b_iug <- RMSE(test_set$rating, predicted_ratings_b_iug)
rmse_b_iug
```
To calculate and apply the time effect, the difference between the release date of a movie and the review date in years will be added to data sets.

```{r add time diff}
train_set <- train_set %>%
  mutate(year_m = str_extract(title, "\\(\\d{4}\\)")) %>%
  mutate(year_m = str_extract(year_m, "\\d{4}")) %>%
  mutate(year_m = as.numeric(year_m),
         year_r = year(as.POSIXct(timestamp, origin="1970-01-01")),
         year_diff = year_r - year_m)

test_set <- test_set %>%
  mutate(year_m = str_extract(title, "\\(\\d{4}\\)")) %>%
  mutate(year_m = str_extract(year_m, "\\d{4}")) %>%
  mutate(year_m = as.numeric(year_m),
         year_r = year(as.POSIXct(timestamp, origin="1970-01-01")),
         year_diff = year_r - year_m)
```

Time effect:

```{r time effect}
b_y <- train_set %>% 
  left_join(b_i, by="movieId") %>%
  left_join(b_u, by = "userId") %>%
  left_join(b_g, by = "genres") %>%
  group_by(year_diff) %>%
  summarize(b_y = sum(rating - b_i - b_u - b_g - mu)/n())

predicted_ratings_b_iugy <- 
  test_set %>% 
  left_join(b_i, by = "movieId") %>%
  left_join(b_u, by = "userId") %>%
  left_join(b_g, by = "genres") %>%
  left_join(b_y, by = "year_diff") %>%
  mutate(pred = mu + b_i + b_u + b_g + b_y) %>%
  pull(pred)

rmse_b_iugy <- RMSE(test_set$rating, predicted_ratings_b_iugy)
rmse_b_iugy
```
All data is summarized in a tibble with the following code:

```{r summary tibble}
results <- tibble(method = c("Random", 
                                 "Mean", 
                                 "Movie Effect", 
                                 "Movie + User Effect", 
                                 "Movie + User + Genre Effect",
                                 "Movie + User + Genre + Time Effect"),
                      RMSE = c(rmse_random,
                               rmse_mean,
                               rmse_b_i,
                               rmse_b_iu,
                               rmse_b_iug,
                               rmse_b_iugy))

results <- results %>%
  mutate(improvement = (1 - RMSE/lag(RMSE))*100)
results
```
An improvement of RMSE is observed for addition of each effect, though in a smaller extent for the last two effects.


## Model with regularization

In order to decrease RMSE even further, regularization of the effects will be introduced according to the equations 10--13. Penalty terms will be defined step-by-step for all effects.

Movie effect with regularization:

```{r movie effect reg}
lambdas1 <- seq(0, 10, 0.25)

rmses_b_i <- sapply(lambdas1, function(l){
  
  b_i <- train_set %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  
  predicted_ratings <- test_set %>% 
    left_join(b_i, by = "movieId") %>%
    mutate(pred = mu + b_i) %>%
    pull(pred)
  
  return(RMSE(test_set$rating, predicted_ratings))
})
```

The influence of the penalty term value on RMSE can be shown in the following plot.

```{r echo=FALSE}
data.frame(lambda = lambdas1, RMSE = rmses_b_i) %>%
  ggplot(aes(lambda, RMSE)) +
  geom_point()
```

The best __\lambda__ is defined and the lowest RMSE is shown. A vector of movie effects based on the best __\lambda__ is saved for the final evaluation.

```{r lambda1}
lambda1 <- lambdas1[which.min(rmses_b_i)]
lambda1

rmse_b_i_reg <- min(rmses_b_i)
rmse_b_i

b_i_reg <- train_set %>% 
  group_by(movieId) %>%
  summarize(b_i_reg = sum(rating - mu)/(n()+lambda1))
```
User effect with regularization:

```{r user effect reg}
lambdas2 <- seq(0, 10, 0.25)

rmses_b_iu <- sapply(lambdas2, function(l){
  
  b_u <- train_set %>% 
    left_join(b_i_reg, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i_reg - mu)/(n()+l))
  
  predicted_ratings <- test_set %>% 
    left_join(b_i_reg, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i_reg + b_u) %>%
    pull(pred)
  
  return(RMSE(test_set$rating, predicted_ratings))
})
```

```{r echo=FALSE}
data.frame(lambda = lambdas2, RMSE = rmses_b_iu) %>%
  ggplot(aes(lambda, RMSE)) +
  geom_point()
```

```{r}
lambda2 <- lambdas2[which.min(rmses_b_iu)]
lambda2

rmse_b_iu_reg <- min(rmses_b_iu)
rmse_b_iu_reg

b_u_reg <- train_set %>% 
  left_join(b_i_reg, by="movieId") %>%
  group_by(userId) %>%
  summarize(b_u_reg = sum(rating - b_i_reg - mu)/(n()+lambda2))
```

Genre effect with regularization: 

```{r genre effect reg}
lambdas3 <- seq(0, 10, 0.25)

rmses_b_iug <- sapply(lambdas3, function(l){
  
  b_g <- train_set %>% 
    left_join(b_i_reg, by="movieId") %>%
    left_join(b_u_reg, by = "userId") %>%
    group_by(genres) %>%
    summarize(b_g = sum(rating - b_i_reg - b_u_reg - mu)/(n()+l))
  
  predicted_ratings <- test_set %>% 
    left_join(b_i_reg, by = "movieId") %>%
    left_join(b_u_reg, by = "userId") %>%
    left_join(b_g, by = "genres") %>%
    mutate(pred = mu + b_i_reg + b_u_reg + b_g) %>%
    pull(pred)
  
  return(RMSE(test_set$rating, predicted_ratings))
})
```

```{r}
data.frame(lambda = lambdas3, RMSE = rmses_b_iug) %>%
  ggplot(aes(lambda, RMSE)) +
  geom_point()
```

```{r}
lambda3 <- lambdas3[which.min(rmses_b_iug)]
lambda3

rmse_b_iug_reg <- min(rmses_b_iug)
rmse_b_iug_reg

b_g_reg <- train_set %>% 
  left_join(b_i_reg, by = "movieId") %>%
  left_join(b_u_reg, by = "userId") %>%
  group_by(genres) %>%
  summarize(b_g_reg = sum(rating - b_i_reg - b_u_reg - mu)/(n()+lambda3))
```
Time effect with regularization:

```{r time effect reg}
lambdas4 <- seq(300, 400, 10)

rmses_b_iugy <- sapply(lambdas4, function(l){

  b_y <- train_set %>% 
    left_join(b_i_reg, by="movieId") %>%
    left_join(b_u_reg, by = "userId") %>%
    left_join(b_g_reg, by = "genres") %>%
    group_by(year_diff) %>%
    summarize(b_y = sum(rating - b_i_reg - b_u_reg - b_g_reg - mu)/(n()+l))
  
  predicted_ratings <-  test_set %>% 
    left_join(b_i_reg, by = "movieId") %>%
    left_join(b_u_reg, by = "userId") %>%
    left_join(b_g_reg, by = "genres") %>%
    left_join(b_y, by = "year_diff") %>%
    mutate(pred = mu + b_i_reg + b_u_reg + b_g_reg + b_y) %>%
    pull(pred)
  
  return(RMSE(test_set$rating, predicted_ratings))
})
```

Since the parameter year difference presents a relative small number of groups, the sample number is each group is very high. Therefore, the regularization for this parameter is less important and the value of __\lambda__ is relative high.

```{r}
data.frame(lambda = lambdas4, RMSE = rmses_b_iugy) %>%
  ggplot(aes(lambda, RMSE)) +
  geom_point()
```

```{r}
lambda4 <- lambdas4[which.min(rmses_b_iugy)]
lambda4

rmse_b_iugy_reg <- min(rmses_b_iugy)
rmse_b_iugy_reg

b_y_reg <- train_set %>% 
  left_join(b_i_reg, by = "movieId") %>%
  left_join(b_u_reg, by = "userId") %>%
  left_join(b_g_reg, by = "genres") %>%
  group_by(year_diff) %>%
  summarize(b_y_reg = sum(rating - b_i_reg - b_u_reg - b_g_reg - mu)/(n()+lambda4))
```
Improvement achieved by the introduction of the regularization is shown below (as percent). Though it might not seem significant, the improvement is comparable with the improvement through the introduction of the genre effect and time effect themselves. 

```{r reg imrovement}
(1 - rmse_b_iugy_reg/rmse_b_iugy) * 100
```

## Fine tuning

Finally, some small corrections can be applied to further improve the predictions. Since the model calculates ratings as a sum of several parameters, the ratings lie ipso facto not in the range between 0.5 and 5.0 stars. The actual range is: `r range(predicted_ratings_reg)`. To correct this the ratings below 0.5 will be substituted with 0.5 and the ratings above 5.0 with 5.0 with the following code.

```{r correction}
predicted_ratings_reg_corr <- predicted_ratings_reg
predicted_ratings_reg_corr[predicted_ratings_reg_corr > 5] <- 5
predicted_ratings_reg_corr[predicted_ratings_reg_corr < 0.5] <- 0.5

rmse_b_iugy_reg_final <- RMSE(test_set$rating, predicted_ratings_reg_corr)
rmse_b_iugy_reg_final
```
Moreover, since the actual ratings are presented between 0.5 and 5.0 stars with the step 0.5, one can assume that rounding the predicted ratings to the nearest .5 or .0 values could also result in an improvement. However, this approach shows contrariwise a considerable increase of RMSE and will not be applied in the final fine tuning.

```{r rounding}
predicted_ratings_reg_corr2 <- plyr::round_any(predicted_ratings_reg_corr, 0.5)

RMSE(test_set$rating, predicted_ratings_reg_corr2)
```
All new results are added to the summary tibble.

```{r summary with reg, rows.print = 11}
results_reg <- tibble(method = c("Movie Effect Reg", 
                             "Movie + User Effect Reg", 
                             "Movie + User + Genre Effect Reg",
                             "Movie + User + Genre + Time Effect Reg",
                             "All Reg + Correction"),
                  RMSE = c(rmse_b_i_reg,
                           rmse_b_iu_reg,
                           rmse_b_iug_reg,
                           rmse_b_iugy_reg,
                           rmse_b_iugy_reg_final))

results_reg <- results_reg %>%
  mutate(improvement = (1 - RMSE/lag(RMSE))*100)
  
results <- rbind(results, results_reg)
print(results)
```

## Model evaluation

Now everything is ready to apply the developed model to the validation set and to find out the final performance of the recommendation system.

The validation set is first amended with the difference between the release date of a movie and the review date in years necessary for the calculation of the time effect.

```{r validation set diff_year}
validation <- validation %>%
  mutate(year_m = str_extract(title, "\\(\\d{4}\\)")) %>%
  mutate(year_m = str_extract(year_m, "\\d{4}")) %>%
  mutate(year_m = as.numeric(year_m),
         year_r = year(as.POSIXct(timestamp, origin="1970-01-01")),
         year_diff = year_r - year_m)
```

Then the effects are recalculated using the whole edx set.

```{r effect calc edx}
edx <- edx %>%
  mutate (year_m = str_extract(title, "\\(\\d{4}\\)")) %>%
  mutate(year_m = str_extract(year_m, "\\d{4}")) %>%
  mutate(year_m = as.numeric(year_m),
         year_r = year(as.POSIXct(timestamp, origin="1970-01-01")),
         year_diff = year_r - year_m)

mu_edx <- mean(edx$rating)

b_i_edx <- edx %>% 
  group_by(movieId) %>%
  summarize(b_i_edx = sum(rating - mu_edx)/(n()+lambda1))

b_u_edx <- edx %>% 
  left_join(b_i_edx, by="movieId") %>%
  group_by(userId) %>%
  summarize(b_u_edx = sum(rating - b_i_edx - mu_edx)/(n()+lambda2))

b_g_edx <- edx %>% 
  left_join(b_i_edx, by = "movieId") %>%
  left_join(b_u_edx, by = "userId") %>%
  group_by(genres) %>%
  summarize(b_g_edx = sum(rating - b_i_edx - b_u_edx - mu_edx)/(n()+lambda3))

b_y_edx <- edx %>% 
  left_join(b_i_edx, by = "movieId") %>%
  left_join(b_u_edx, by = "userId") %>%
  left_join(b_g_edx, by = "genres") %>%
  group_by(year_diff) %>%
  summarize(b_y_edx = sum(rating - b_i_edx - b_u_edx - b_g_edx - mu_edx)/(n()+lambda4))
```

Finally, the ratings of the validation set are predicted based on the developed model, corrected as described above and the final RMSE is calculated.

```{r validation prediction}
predicted_ratings_val <-  validation %>% 
  left_join(b_i_edx, by = "movieId") %>%
  left_join(b_u_edx, by = "userId") %>%
  left_join(b_g_edx, by = "genres") %>%
  left_join(b_y_edx, by = "year_diff") %>%
  mutate(pred = mu_edx + b_i_edx + b_u_edx + b_g_edx + b_y_edx) %>%
  pull(pred)

predicted_ratings_val[predicted_ratings_val > 5] <- 5
predicted_ratings_val[predicted_ratings_val < 0.5] <- 0.5

rmse_val <- RMSE(validation$rating, predicted_ratings_val)
rmse_val
```
The RMSE `r rmse_val` of the prediction on the validation set is close to that of the test set and even smaller. This supports the validity of the developed model and of the applied approach.


# Conclusion


