---
title: "MovieLens project: Creating a movie recommendation system"
author: "Anton Ivanov"
date: "`r format(Sys.Date())`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { 
      equationNumbers: { 
            autoNumber: "all",
            formatNumber: function (n) {return +n}
      } 
  }
});
</script>




# 1 Introduction

In this project, we will be creating a movie recommendation system using the [10M version of the MovieLens dataset](https://grouplens.org/datasets/movielens/10m/).
This dataset contains 10 million ratings for about 10,000 movies from 72,000 users.

Recommendation systems make specific recommendations based on ratings from users to items. They are of particular importance in the modern world, since they allow companies such as online shops or streaming services to recommend users matching goods or content, providing higher profits. 

The goal of the project is to predict a rating for a specific movie from a specific user, based on the available ratings.
The used ratings are as following: one star suggests it is not a good movie, whereas five stars suggest it is an excellent movie.

Since in the dataset only 10 million ratings are available out of potential 78 million (if every user would rate every movie), about 86% of ratings remain unknown and have to be predicted.

The recommendation system will be based on a linear model containing movie, user, genre and time effects. Each effect will be considered step-by-step, using a training and test sets for evaluation of the model performance. Regularization of the effects will be then implemented to improve the performance. After final tuning of the model, it will be applied to the validation set to assess the final performance.


# 2 Analysis and Methods

## 2.1 Data preparation

The 10M Movielens dataset is available for a downloaded as a \*.zip file containing two \*.dat files: movies.dat and ratings.dat. The files contain data of the following features:

* user id: an entry number specific for each user;
* movie id: an entry number specific for each movie;
* rating: movie ratings in the range from 0.5 to 5 stars with the step 0.5 stars;
* time stamp: an integer presented as number of seconds after 1st January 1970;
* title of the movie with release year;
* genre of the movie.

The features are separated by double colons "::". The values are extracted and saved as a dataframe using the following code.

```{r dataset preparation, message=FALSE, warning=FALSE}
library(tidyverse)
library(data.table)
dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")
```

In the resulting dataframe, each row represents a rating by one user to one movie. These are the first 10 rows of the dataframe:

```{r ten rows of dataset, message=FALSE}
as_tibble(movielens)
```


## 2.2 Preparation of the training, test and validation sets

For the final evaluation of the recommendation system's performance, a validation dataset containing 10% of data is created. The rest of the data is saved as a edx subset. It is ensured that the userId and movieId values in the validation set are also present in the edx set.

```{r validation dataset, message=FALSE, warning=FALSE}
library(caret)
set.seed(1, sample.kind="Rounding")
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)
```

The edx dataset is used for the creation and training of the recommendation system and is split into the training set (80%) and test set (20%) for these needs. 

```{r train and test sets}
set.seed(2)
test_index <- createDataPartition(edx$rating, times = 1, p = 0.2, list = FALSE)
train_set <- edx[-test_index]
test_set <- edx[test_index]
test_set <- test_set %>% 
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")
```

## 2.3 Creating a linear model

The linear model will be built based on observations made for the edx dataset. We will start with the simplest model and will gradually increase its complexity.

The simplest model would be just assuming all unknown ratings equal to the average rating of all movies. It can be presented with the following equation.

\begin{equation}
Y_{u,i} = \mu + \epsilon_{u,i}
\end{equation}

where $Y_{u,i}$ is the predicted rating, $\mu$ is a mean of all ratings in the data set and $\epsilon_{u,i}$ is an independent error (it is added as a reminder that the prediction is not perfect).

Of course such assumption would not result in an accurate prediction, as will be shown in the Results Chapter. To improve the prediction, an effect of each specific movie on its rating will be considered. It is obvious that each movie has a different "true" rating --- there are better and worse movies. This can be supported by the following plot showing mean ratings of the movies with more than 500 ratings. No pattern can be observed.

```{r}
edx %>%
  group_by(movieId) %>%
  summarize(mean = mean(rating), n = n()) %>%
  filter(n > 500) %>%
  ggplot(aes(seq(1, length(movieId)), mean)) +
  geom_point() + 
  xlab("entry")
```

This fact will be used to predict ratings. The model will include the average of **all** movies in the dataset plus the term **specific** for each movie:

\begin{equation}
Y_{u,i} = \mu + b_i + \epsilon_{u,i}
\end{equation}

where $b_i$ is the item (movie) effect.

Analogously, similar relationships can be shown for the overall ratings of one user. Each user shows a different overall average rating. This means that on average some users give higher ratings than other users. The following plot depicts mean ratings of users with more than 500 ratings. As expected, no pattern can be observed.

```{r}
edx %>%
  group_by(userId) %>%
  summarize(mean = mean(rating), n = n()) %>%
  filter(n > 500) %>%
  ggplot(aes(seq(1, length(userId)), mean)) +
  geom_point() + 
  xlab("entry")
```

Thus, another term can be included in the model --- the user effect:

\begin{equation}
Y_{u,i} = \mu + b_i + b_u + \epsilon_{u,i}
\end{equation}
where $b_u$ is the user effect.

The shown approach can also be implemented to the genres of movies. This is less intuitive, but it is easy to show that on average some genres get  higher ratings than others.

These are the top 5 rated genres:

```{r}
edx %>%
  group_by(genres) %>%
  summarize(mean_g = mean(rating), n = n()) %>%
  filter(n > 500) %>%
  slice_max(mean_g, n = 5)
```


And the bottom 5 rated genres:

```{r}
edx %>%
  group_by(genres) %>%
  summarize(mean_g = mean(rating), n = n()) %>%
  filter(n > 500) %>%
  slice_min(mean_g, n = 5)
```

Accordingly, the genre effect is also included in the model.

\begin{equation}
Y_{u,i} = \mu + b_i + b_u + b_g + \epsilon_{u,i}
\end{equation}
where $b_g$ is the genre effect.

The last bit of available information in the dataset is the year of the movie release (included in the title) and the date when the rating was given. This information itself might not be very helpful. However, according to our experience, the time __between__ the movie release and the rating date (_year difference_) might be important. It seems that older movie being rated after many years are rated more positive. To check this hypothesis, the dataset will first be adjusted to include the new columns: movie date, rating date and the difference in years between them. Afterwards, the ratings will be plotted against the time difference for a sample of 1,000,000 entries. 

```{r adding year to movielense, message=FALSE}
library(lubridate)

edx <- edx %>%
  mutate(year_m = str_extract(title, "\\(\\d{4}\\)")) %>%
  mutate(year_m = str_extract(year_m, "\\d{4}")) %>%
  mutate(year_m = as.numeric(year_m),
         year_r = year(as.POSIXct(timestamp, origin="1970-01-01")),
         year_diff = year_r - year_m)
```


```{r plot year_diff to rating, message=FALSE}
edx[sample(nrow(edx), 1000000),] %>%
  group_by(year_diff) %>%
  summarize(rating_mean = mean(rating)) %>%
  ggplot(aes(year_diff, rating_mean)) +
  geom_point() +
  geom_smooth(method = "lm")
```

As can be seen, indeed, there is a positive correlation between the movie date and the _year difference_. Similarly to the previous approach, this information will be included in the model via the time effect $b_y$:

\begin{equation}
Y_{u,i} = \mu + b_i + b_u + b_g + b_y + \epsilon_{u,i}
\end{equation}

Now the model includes all useful information available in the dataset presented through the movie, user, genre and time effects. But how do we calculate the effects?

## 2.4 Calculation of effects

We will perform the calculation of all effects step by step starting with the movie effect $b_i$. From the equation 2, the $b_i$ specific for each movie can be approximated based on the average rating of all movies $\mu$ and the **known** ratings of a movie.

\begin{equation}
\hat{b_i} = \sum_{u=1}^{n_i}(Y_{i,u} - \hat{\mu})
\end{equation}

whereas terms with hats are estimated values.

Knowing $b_i$ and rewriting the equation 3 the effect $b_u$ can be found as following. 

\begin{equation}
\hat{b_u} = \sum_{u=1}^{n_i}(Y_{i,u} - \hat{\mu} - \hat{b_i})
\end{equation}

In the same manner, the values of the last two effects can be found based on equations 4-5.

\begin{equation}
\hat{b_g} = \sum_{u=1}^{n_i}(Y_{i,u} - \hat{\mu} - \hat{b_i} - \hat{b_u})
\end{equation}

\begin{equation}
\hat{b_y} = \sum_{u=1}^{n_i}(Y_{i,u} - \hat{\mu} - \hat{b_i} - \hat{b_u} - \hat{b_g})
\end{equation}


## 2.5 Calculation of effects with regularization

As shown in equations 6-9, calculation of effects depends on averages of ratings grouped in different ways (ratings for the same movie, of the same user, of the same genre or of the same movie date and rating date difference). This means that the more ratings of one group are available, the better estimation of an effect is possible. To compensate a strong influence of low number of ratings, a penalty term $\lambda$ is introduced into the equations for the effects. Penalizing large estimates that are formed using small sample sizes is the concept of **regularization**.

For example, for the movie effect $b_i$, the modified equation looks as following:

\begin{equation}
\hat{b_i}(\lambda) = \frac{1}{\lambda + n_i}\sum_{u=1}^{n_i}(Y_{i,u} - \hat{\mu})
\end{equation}

As can be seen, when the sample size $n_i$ is large giving a stable estimate of $\hat{b_i}$, the penalty term $\lambda$ is ignored and $n_i + \lambda \approx n_i$. By a small sample size $\lambda$ becomes dominant and $\hat{b_i}(\lambda)$ is shrunken towards 0.

In a similar way, penalty terms will be added to each effect. Optimal penalty terms will be defined by trying out different values and comparing their influence on the performance of the prediction. 

## 2.6 Loss function

The performance of the prediction system will be measured by the typical error loss --- the residual mean squared error (RMSE) on a test set:

\begin{equation}
RMSE = \sqrt{\frac{1}{N}\sum_{u,i}(\hat{y}_{u,i}-y_{u,i})^2}
\end{equation}

where $y_{u,i}$ is an actual rating of user __u__ for movie __i__, $\hat{y}_{u,i}$ is the corresponding predicted value and __N__ is  the number of user/movie combinations.

The function that will compute the RMSE for vectors of ratings and their corresponding predictors:

```{r RMSE}
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
```


# 3 Results

In the following, the described model will be built and the effects and regularization penalty terms will be calculated based on the train and test sets. The final performance of the recommendation system will be evaluated with the validation set.

## 3.1 Model without regularization

First, we will try just to randomly guess the ratings and show the corresponding RMSE:

```{r guessing}
set.seed(3)
pred_random <- sample(seq(0.5, 5, by = 0.5), nrow(test_set), replace = TRUE)
rmse_random <- RMSE(test_set$rating, pred_random)
rmse_random
```
Not surprisingly, the RMSE is quite high. Just applying our simplest model --- assuming all ratings to be the average of the known ratings --- already improves the performance:

```{r just average}
mu <- mean(train_set$rating)
rmse_mean <- RMSE(test_set$rating, mu)
rmse_mean
```
Now we will add the first effect (movie effect) to the model. The effect is calculated according to the equation 6:

```{r movie effect}
b_i <- train_set %>% 
  group_by(movieId) %>%
  summarize(b_i = sum(rating - mu)/n())

predicted_ratings_b_i <- test_set %>% 
  left_join(b_i, by = "movieId") %>%
  mutate(pred = mu + b_i) %>%
  pull(pred)

rmse_b_i <- RMSE(test_set$rating, predicted_ratings_b_i)
rmse_b_i
```
The addition of the movie effect results in the improvement of the prediction on the test set of 11%. This is a considerable improvement of the performance. The implementation of the other effects in the model and their influence on the model performance are shown in the following.

User effect:

```{r user effect}
b_u <- train_set %>% 
  left_join(b_i, by="movieId") %>%
  group_by(userId) %>%
  summarize(b_u = sum(rating - b_i - mu)/n())

predicted_ratings_b_iu <- test_set %>% 
  left_join(b_i, by = "movieId") %>%
  left_join(b_u, by = "userId") %>%
  mutate(pred = mu + b_i + b_u) %>%
  pull(pred)

rmse_b_iu <- RMSE(test_set$rating, predicted_ratings_b_iu)
rmse_b_iu
```
Genre effect:

```{r genre effect}
b_g <- train_set %>% 
  left_join(b_i, by="movieId") %>%
  left_join(b_u, by = "userId") %>%
  group_by(genres) %>%
  summarize(b_g = sum(rating - b_i - b_u - mu)/n())

predicted_ratings_b_iug <- 
  test_set %>% 
  left_join(b_i, by = "movieId") %>%
  left_join(b_u, by = "userId") %>%
  left_join(b_g, by = "genres") %>%
  mutate(pred = mu + b_i + b_u + b_g) %>%
  pull(pred)

rmse_b_iug <- RMSE(test_set$rating, predicted_ratings_b_iug)
rmse_b_iug
```
To calculate and apply the time effect, the _year difference_ will be added to the data sets.

```{r add time diff}
train_set <- train_set %>%
  mutate(year_m = str_extract(title, "\\(\\d{4}\\)")) %>%
  mutate(year_m = str_extract(year_m, "\\d{4}")) %>%
  mutate(year_m = as.numeric(year_m),
         year_r = year(as.POSIXct(timestamp, origin="1970-01-01")),
         year_diff = year_r - year_m)

test_set <- test_set %>%
  mutate(year_m = str_extract(title, "\\(\\d{4}\\)")) %>%
  mutate(year_m = str_extract(year_m, "\\d{4}")) %>%
  mutate(year_m = as.numeric(year_m),
         year_r = year(as.POSIXct(timestamp, origin="1970-01-01")),
         year_diff = year_r - year_m)
```

Time effect:

```{r time effect}
b_y <- train_set %>% 
  left_join(b_i, by="movieId") %>%
  left_join(b_u, by = "userId") %>%
  left_join(b_g, by = "genres") %>%
  group_by(year_diff) %>%
  summarize(b_y = sum(rating - b_i - b_u - b_g - mu)/n())

predicted_ratings_b_iugy <- 
  test_set %>% 
  left_join(b_i, by = "movieId") %>%
  left_join(b_u, by = "userId") %>%
  left_join(b_g, by = "genres") %>%
  left_join(b_y, by = "year_diff") %>%
  mutate(pred = mu + b_i + b_u + b_g + b_y) %>%
  pull(pred)

rmse_b_iugy <- RMSE(test_set$rating, predicted_ratings_b_iugy)
rmse_b_iugy
```
All data are summarized in a table with the following code:

```{r summary tibble}
results <- tibble(method = c("Random", 
                                 "Mean", 
                                 "Movie Effect", 
                                 "Movie + User Effect", 
                                 "Movie + User + Genre Effect",
                                 "Movie + User + Genre + Time Effect"),
                      RMSE = c(rmse_random,
                               rmse_mean,
                               rmse_b_i,
                               rmse_b_iu,
                               rmse_b_iug,
                               rmse_b_iugy))

results <- results %>%
  mutate(improvement = (1 - RMSE/lag(RMSE))*100)
knitr::kable(results)
```
An improvement of RMSE is observed for the addition of each effect, though in a smaller extent for the last two effects.


## 3.2 Model with regularization

In order to decrease RMSE even further, regularization of the effects will be introduced according to the equation 10. Penalty terms will be defined step-by-step for all effects.

Movie effect with regularization:

```{r movie effect reg}
lambdas1 <- seq(0, 10, 0.25)

rmses_b_i <- sapply(lambdas1, function(l){
  
  b_i <- train_set %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  
  predicted_ratings <- test_set %>% 
    left_join(b_i, by = "movieId") %>%
    mutate(pred = mu + b_i) %>%
    pull(pred)
  
  return(RMSE(test_set$rating, predicted_ratings))
})
```

The influence of the penalty term value on RMSE can be shown in the following plot:

```{r echo=FALSE}
data.frame(lambda = lambdas1, RMSE = rmses_b_i) %>%
  ggplot(aes(lambda, RMSE)) +
  geom_point()
```

The best $\lambda$ is defined and the lowest RMSE is shown. A vector of movie effects based on the best $\lambda$ is saved for the final evaluation.

```{r lambda1}
lambda1 <- lambdas1[which.min(rmses_b_i)]
lambda1

rmse_b_i_reg <- min(rmses_b_i)
rmse_b_i

b_i_reg <- train_set %>% 
  group_by(movieId) %>%
  summarize(b_i_reg = sum(rating - mu)/(n()+lambda1))
```
User effect with regularization:

```{r user effect reg}
lambdas2 <- seq(0, 10, 0.25)

rmses_b_iu <- sapply(lambdas2, function(l){
  
  b_u <- train_set %>% 
    left_join(b_i_reg, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i_reg - mu)/(n()+l))
  
  predicted_ratings <- test_set %>% 
    left_join(b_i_reg, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i_reg + b_u) %>%
    pull(pred)
  
  return(RMSE(test_set$rating, predicted_ratings))
})
```

```{r echo=FALSE}
data.frame(lambda = lambdas2, RMSE = rmses_b_iu) %>%
  ggplot(aes(lambda, RMSE)) +
  geom_point()
```

```{r}
lambda2 <- lambdas2[which.min(rmses_b_iu)]
lambda2

rmse_b_iu_reg <- min(rmses_b_iu)
rmse_b_iu_reg

b_u_reg <- train_set %>% 
  left_join(b_i_reg, by="movieId") %>%
  group_by(userId) %>%
  summarize(b_u_reg = sum(rating - b_i_reg - mu)/(n()+lambda2))
```

Genre effect with regularization: 

```{r genre effect reg}
lambdas3 <- seq(0, 10, 0.25)

rmses_b_iug <- sapply(lambdas3, function(l){
  
  b_g <- train_set %>% 
    left_join(b_i_reg, by="movieId") %>%
    left_join(b_u_reg, by = "userId") %>%
    group_by(genres) %>%
    summarize(b_g = sum(rating - b_i_reg - b_u_reg - mu)/(n()+l))
  
  predicted_ratings <- test_set %>% 
    left_join(b_i_reg, by = "movieId") %>%
    left_join(b_u_reg, by = "userId") %>%
    left_join(b_g, by = "genres") %>%
    mutate(pred = mu + b_i_reg + b_u_reg + b_g) %>%
    pull(pred)
  
  return(RMSE(test_set$rating, predicted_ratings))
})
```

```{r}
data.frame(lambda = lambdas3, RMSE = rmses_b_iug) %>%
  ggplot(aes(lambda, RMSE)) +
  geom_point()
```

```{r}
lambda3 <- lambdas3[which.min(rmses_b_iug)]
lambda3

rmse_b_iug_reg <- min(rmses_b_iug)
rmse_b_iug_reg

b_g_reg <- train_set %>% 
  left_join(b_i_reg, by = "movieId") %>%
  left_join(b_u_reg, by = "userId") %>%
  group_by(genres) %>%
  summarize(b_g_reg = sum(rating - b_i_reg - b_u_reg - mu)/(n()+lambda3))
```
Time effect with regularization:

```{r time effect reg}
lambdas4 <- seq(300, 400, 10)

rmses_b_iugy <- sapply(lambdas4, function(l){

  b_y <- train_set %>% 
    left_join(b_i_reg, by="movieId") %>%
    left_join(b_u_reg, by = "userId") %>%
    left_join(b_g_reg, by = "genres") %>%
    group_by(year_diff) %>%
    summarize(b_y = sum(rating - b_i_reg - b_u_reg - b_g_reg - mu)/(n()+l))
  
  predicted_ratings <-  test_set %>% 
    left_join(b_i_reg, by = "movieId") %>%
    left_join(b_u_reg, by = "userId") %>%
    left_join(b_g_reg, by = "genres") %>%
    left_join(b_y, by = "year_diff") %>%
    mutate(pred = mu + b_i_reg + b_u_reg + b_g_reg + b_y) %>%
    pull(pred)
  
  return(RMSE(test_set$rating, predicted_ratings))
})
```

Since the parameter _year difference_ presents a relatively small number of groups, the sample number of each group is very high. Therefore, the regularization for this parameter is less important and the value of $\lambda$ is relative high.

```{r}
data.frame(lambda = lambdas4, RMSE = rmses_b_iugy) %>%
  ggplot(aes(lambda, RMSE)) +
  geom_point()
```

```{r}
lambda4 <- lambdas4[which.min(rmses_b_iugy)]
lambda4

rmse_b_iugy_reg <- min(rmses_b_iugy)
rmse_b_iugy_reg

b_y_reg <- train_set %>% 
  left_join(b_i_reg, by = "movieId") %>%
  left_join(b_u_reg, by = "userId") %>%
  left_join(b_g_reg, by = "genres") %>%
  group_by(year_diff) %>%
  summarize(b_y_reg = sum(rating - b_i_reg - b_u_reg - b_g_reg - mu)/(n()+lambda4))
```
Improvement achieved by the introduction of the regularization is shown below (as percent). Though it might not seem significant, the improvement is comparable with the improvement through the introduction of the genre effect and time effect themselves. 

```{r reg imrovement}
(1 - rmse_b_iugy_reg/rmse_b_iugy) * 100
```

Now predicted ratings based on all effects can be calculated and used for the fine tunung.

```{r predicted ratings reg}
predicted_ratings_reg <-  test_set %>% 
  left_join(b_i_reg, by = "movieId") %>%
  left_join(b_u_reg, by = "userId") %>%
  left_join(b_g_reg, by = "genres") %>%
  left_join(b_y_reg, by = "year_diff") %>%
  mutate(pred = mu + b_i_reg + b_u_reg + b_g_reg + b_y_reg) %>%
  pull(pred)
```


## 3.3 Fine tuning

Finally, some small corrections can be applied to further improve the predictions. Since the model calculates ratings as a sum of several parameters, the ratings lie ipso facto not in the range between 0.5 and 5.0 stars. The range for the current calculation is: `r range(predicted_ratings_reg)`. To correct this, the ratings below 0.5 will be substituted with 0.5 and the ratings above 5.0 with 5.0 with the following code.

```{r correction}
predicted_ratings_reg_corr <- predicted_ratings_reg
predicted_ratings_reg_corr[predicted_ratings_reg_corr > 5] <- 5
predicted_ratings_reg_corr[predicted_ratings_reg_corr < 0.5] <- 0.5

rmse_b_iugy_reg_final <- RMSE(test_set$rating, predicted_ratings_reg_corr)
rmse_b_iugy_reg_final
```
As can be seen, this correction results in an improved RMSE.

Moreover, since the actual ratings are presented between 0.5 and 5.0 stars with the step 0.5, one can assume that rounding the predicted ratings to the nearest .5 or .0 values could also result in an improvement. However, this approach shows a considerable increase of RMSE and will not be applied in the final fine tuning.

```{r rounding}
predicted_ratings_reg_corr2 <- plyr::round_any(predicted_ratings_reg_corr, 0.5)

RMSE(test_set$rating, predicted_ratings_reg_corr2)
```
All new results including the effect regularization and the fine tuning are added to the summary table.

```{r summary with reg, rows.print = 11}
results_reg <- tibble(method = c("Movie Effect Reg", 
                             "Movie + User Effect Reg", 
                             "Movie + User + Genre Effect Reg",
                             "Movie + User + Genre + Time Effect Reg",
                             "All Reg + Correction"),
                  RMSE = c(rmse_b_i_reg,
                           rmse_b_iu_reg,
                           rmse_b_iug_reg,
                           rmse_b_iugy_reg,
                           rmse_b_iugy_reg_final))

results_reg <- results_reg %>%
  mutate(improvement = (1 - RMSE/lag(RMSE))*100)
  
results <- rbind(results, results_reg)
knitr::kable(results)
```

## 3.4 Model evaluation

Now everything is ready to apply the developed model to the validation set and to find out the final performance of the recommendation system.

First, the validation set is amended with the _year difference_ necessary for the calculation of the time effect.

```{r validation set diff_year}
validation <- validation %>%
  mutate(year_m = str_extract(title, "\\(\\d{4}\\)")) %>%
  mutate(year_m = str_extract(year_m, "\\d{4}")) %>%
  mutate(year_m = as.numeric(year_m),
         year_r = year(as.POSIXct(timestamp, origin="1970-01-01")),
         year_diff = year_r - year_m)
```

Then the effects are recalculated using the whole edx set.

```{r effect calc edx}
mu_edx <- mean(edx$rating)

b_i_edx <- edx %>% 
  group_by(movieId) %>%
  summarize(b_i_edx = sum(rating - mu_edx)/(n()+lambda1))

b_u_edx <- edx %>% 
  left_join(b_i_edx, by="movieId") %>%
  group_by(userId) %>%
  summarize(b_u_edx = sum(rating - b_i_edx - mu_edx)/(n()+lambda2))

b_g_edx <- edx %>% 
  left_join(b_i_edx, by = "movieId") %>%
  left_join(b_u_edx, by = "userId") %>%
  group_by(genres) %>%
  summarize(b_g_edx = sum(rating - b_i_edx - b_u_edx - mu_edx)/(n()+lambda3))

b_y_edx <- edx %>% 
  left_join(b_i_edx, by = "movieId") %>%
  left_join(b_u_edx, by = "userId") %>%
  left_join(b_g_edx, by = "genres") %>%
  group_by(year_diff) %>%
  summarize(b_y_edx = sum(rating - b_i_edx - b_u_edx - b_g_edx - mu_edx)/(n()+lambda4))
```

Finally, the ratings of the validation set are predicted based on the developed model, corrected as described above and the final RMSE is calculated.

```{r validation prediction}
predicted_ratings_val <-  validation %>% 
  left_join(b_i_edx, by = "movieId") %>%
  left_join(b_u_edx, by = "userId") %>%
  left_join(b_g_edx, by = "genres") %>%
  left_join(b_y_edx, by = "year_diff") %>%
  mutate(pred = mu_edx + b_i_edx + b_u_edx + b_g_edx + b_y_edx) %>%
  pull(pred)

predicted_ratings_val[predicted_ratings_val > 5] <- 5
predicted_ratings_val[predicted_ratings_val < 0.5] <- 0.5

rmse_val <- RMSE(validation$rating, predicted_ratings_val)
rmse_val
```
The RMSE `r rmse_val` of the prediction on the validation set is close to that of the test set and even smaller. This supports the validity of the developed model and of the applied approach.


# 4 Conclusion

In this project, we have created a recommendation system for prediction of movie ratings. Information used for predictions is as following: user id, movie id, known ratings, year of rating, release year and genre of the movie. 

The recommendation system is based on a linear model including movie, user, genre and time effects. The influence of the effects on the predicted rating is controlled via regularization. The effects were defined using the edx set, the penalty terms were defined using the training and test sets. The final evaluation was performed using the separate validation set.

Recommendation system provided prediction with the performance defined by RMSE: `r rmse_val`. 
